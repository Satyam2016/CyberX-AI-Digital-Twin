# -*- coding: utf-8 -*-
"""Session_hijacking

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1218xvCfPFTHeQyHmWbD1sr90D2w0mPqw
"""


from google.colab import drive
drive.mount('/content/drive')

"""# **Prediction**"""

import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer

# Load dataset
data_path = '/content/drive/MyDrive/LDAP.csv'
df = pd.read_csv(data_path)

# Strip leading/trailing spaces from column names
df.columns = df.columns.str.strip()

# Check for relevant columns and adapt
# Assuming 'logs' can be derived from other session-related columns (e.g., 'Flow ID', 'Source IP', etc.)
required_columns = ['Flow ID', 'Source IP', 'Destination IP', 'Protocol']

# Ensure the required columns exist
missing_columns = [col for col in required_columns if col not in df.columns]
if missing_columns:
    raise KeyError(f"The following required columns are missing: {missing_columns}")

# Combine relevant columns into a single 'logs' column
df['logs'] = df[required_columns].astype(str).agg(' '.join, axis=1)

# Use 'Label' column for binary classification (e.g., 'Normal' = 0, 'Hijacked' = 1)
# Replace 'Label' values as needed if it's not already binary
if 'Label' not in df.columns:
    raise KeyError("'Label' column is missing in the dataset. Please check the file.")

df['label'] = df['Label'].apply(lambda x: 1 if x == 'Hijacked' else 0)

# Split data into training and test sets
train_texts, test_texts, train_labels, test_labels = train_test_split(
    df['logs'], df['label'], test_size=0.2, random_state=42
)

# Tokenize using BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=128, return_tensors="pt")
test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, max_length=128, return_tensors="pt")

print("Tokenization complete. Ready for model training.")

"""# **modify**"""

print("Unique values in the Label column:", df['Label'].unique())

# Map labels to binary classes
df['label'] = df['Label'].apply(
    lambda x: 1 if x in ['NetBIOS', 'LDAP'] else 0  # Adjust as per your context
)

print("Label distribution:\n", df['label'].value_counts())

# Separate classes
class_1 = df[df['label'] == 1]
class_0 = df[df['label'] == 0]

# Undersample class 1
undersampled_class_1 = class_1.sample(len(class_0), random_state=42)

# Combine and shuffle
balanced_df = pd.concat([class_0, undersampled_class_1]).sample(frac=1, random_state=42).reset_index(drop=True)

print("Balanced dataset distribution:\n", balanced_df['label'].value_counts())

# Create a smaller balanced dataset
balanced_class_1 = class_1.sample(5000, random_state=42)  # Take 5000 samples from class 1
balanced_class_0 = class_0.sample(5000, random_state=42)  # Take 5000 samples from class 0

# Combine and shuffle
small_balanced_df = pd.concat([balanced_class_0, balanced_class_1]).sample(frac=1, random_state=42).reset_index(drop=True)

print("Smaller balanced dataset distribution:\n", small_balanced_df['label'].value_counts())

# Split the dataset into train and test
train_texts, test_texts, train_labels, test_labels = train_test_split(
    balanced_df['logs'], balanced_df['label'], test_size=0.2, random_state=42
)

# Tokenize and train the model as before
train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=128, return_tensors="pt")
test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, max_length=128, return_tensors="pt")

import torch

# Define a custom dataset class
class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            'input_ids': self.encodings['input_ids'][idx],
            'attention_mask': self.encodings['attention_mask'][idx],
            'labels': torch.tensor(self.labels.iloc[idx], dtype=torch.long)
        }

# Convert tokenized data to PyTorch datasets
train_dataset = CustomDataset(train_encodings, train_labels)
test_dataset = CustomDataset(test_encodings, test_labels)

print("Datasets prepared for training.")

from transformers import DistilBertForSequenceClassification

# Load the model with a classification head
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)

print("Model loaded successfully.")

from transformers import TrainingArguments

# Define training arguments
training_args = TrainingArguments(
    output_dir='./results',         # Directory to save checkpoints
    num_train_epochs=3,             # Number of epochs
    per_device_train_batch_size=16, # Batch size for training
    per_device_eval_batch_size=16,  # Batch size for evaluation
    evaluation_strategy="epoch",    # Evaluate at the end of each epoch
    save_strategy="epoch",          # Save the model after every epoch
    logging_dir='./logs',           # Directory for logs
    logging_steps=100,              # Log training progress every 100 steps
    save_total_limit=2,             # Limit number of checkpoints
)

from transformers import Trainer

# Initialize the Trainer
trainer = Trainer(
    model=model,                         # Model to train
    args=training_args,                  # Training configuration
    train_dataset=train_dataset,         # Training dataset
    eval_dataset=test_dataset            # Evaluation dataset
)

# Start training
trainer.train()

print("Model training complete.")

# Evaluate the model
eval_results = trainer.evaluate()

print("Evaluation results:", eval_results)

# Save the model and tokenizer
model.save_pretrained('./fine_tuned_distilbert')
tokenizer.save_pretrained('./fine_tuned_distilbert')

print("Model and tokenizer saved.")

import torch
from transformers import Trainer

# ... (your existing code) ...

# Test inputs for both class 0 and class 1
new_texts = [
    "Session hijacking detected with unusual IP changes.",  # Suspicious, expected class: 1
    "User logged in successfully from a known IP address.",  # Normal, expected class: 0
    "Suspicious login attempt from unknown IP address.",      # Suspicious, expected class: 1
    "Session ended gracefully without issues.",              # Normal, expected class: 0
    "Token theft detected with session continuation.",        # Suspicious, expected class: 1
    "Normal session log with no suspicious activity."         # Normal, expected class: 0
]

# Tokenize the new texts
new_encodings = tokenizer(new_texts, truncation=True, padding=True, max_length=128, return_tensors="pt")

# Remove 'token_type_ids' from the encoding dictionary
if 'token_type_ids' in new_encodings:
    del new_encodings['token_type_ids']

# Get the device the model is on
device = next(model.parameters()).device  # Get the device of the model's parameters

# Move the input tensors to the same device as the model
new_encodings = new_encodings.to(device)

# Predict the classes
outputs = model(**new_encodings)
logits = outputs.logits
probabilities = torch.nn.functional.softmax(logits, dim=-1)

# Print predictions
for i, text in enumerate(new_texts):
    predicted_class = torch.argmax(probabilities[i]).item()
    print(f"Log: {text}")
    print(f"Predicted Class: {predicted_class} (Probability: {probabilities[i]})\n")

"""# **Modify2**"""

print(df[required_columns].head())

# Assuming df is your DataFrame

# Get a list of numeric column names
numeric_columns = df.select_dtypes(include=['number']).columns.tolist()

# Now you can use numeric_columns to select the numeric columns from your DataFrame
print(df[numeric_columns].isnull().sum())

df[numeric_columns] = df[numeric_columns].fillna(0)

df = df.dropna(subset=numeric_columns)

import ipaddress
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Convert IP addresses to integers
df['Source_IP_Num'] = df['Source IP'].apply(lambda x: int(ipaddress.IPv4Address(x)))
df['Destination_IP_Num'] = df['Destination IP'].apply(lambda x: int(ipaddress.IPv4Address(x)))

# Extract numeric components from Flow ID
df[['Flow_ID_Part1', 'Flow_ID_Part2', 'Flow_ID_Part3', 'Flow_ID_Part4', 'Flow_ID_Part5']] = \
    df['Flow ID'].str.split('-', expand=True)

# Ensure extracted components are numeric
for col in ['Flow_ID_Part1', 'Flow_ID_Part2', 'Flow_ID_Part3', 'Flow_ID_Part4', 'Flow_ID_Part5']:
    df[col] = pd.to_numeric(df[col], errors='coerce')

# Check for NaN values
print("Missing values in numeric columns:\n", df.isnull().sum())

# Handle missing values by filling with 0
df.fillna(0, inplace=True)

# Select numeric columns for scaling
numeric_columns = ['Source_IP_Num', 'Destination_IP_Num', 'Protocol',
                   'Flow_ID_Part1', 'Flow_ID_Part2', 'Flow_ID_Part3', 'Flow_ID_Part4', 'Flow_ID_Part5']

# Feature Scaling
scaler = MinMaxScaler()
scaled_features = scaler.fit_transform(df[numeric_columns])

# Dimensionality Reduction for Visualization
pca = PCA(n_components=2)
reduced_features = pca.fit_transform(scaled_features)

# Analyze PCA Components
plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=df['label'], cmap='coolwarm', alpha=0.7)
plt.title("PCA Visualization of Session Logs")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.colorbar(label="Label")
plt.show()

from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments
import torch

# Load GPT-2
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")

# Set the padding token to the EOS token
tokenizer.pad_token = tokenizer.eos_token

# Use a smaller subset of the dataset
train_texts = train_texts[:2000]  # Limit dataset to 2000 samples for faster training

# Tokenize dataset with reduced sequence length
train_encodings = tokenizer(
    train_texts,
    truncation=True,
    padding=True,
    max_length=32,  # Reduce sequence length
    return_tensors="pt"
)

# Create Dataset
class GPT2Dataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __len__(self):
        return len(self.encodings['input_ids'])

    def __getitem__(self, idx):
        item = {key: tensor[idx] for key, tensor in self.encodings.items()}
        item['labels'] = item['input_ids'].clone()
        item['labels'][item['labels'] == tokenizer.pad_token_id] = -100  # Ignore padding in loss calculation
        return item

train_dataset = GPT2Dataset(train_encodings)

# Ensure only the output head is trainable
for param in model.transformer.parameters():
    param.requires_grad = False
for param in model.lm_head.parameters():
    param.requires_grad = True

# Define Training Arguments
training_args = TrainingArguments(
    output_dir='./gpt2_session_logs',
    num_train_epochs=1,  # Reduce epochs
    per_device_train_batch_size=8,  # Batch size of 8
    gradient_accumulation_steps=4,  # Accumulate gradients to simulate larger batch size
    save_steps=400,  # Save the model after 400 steps
    save_total_limit=1,  # Limit the number of saved checkpoints
    logging_dir='./logs',
    fp16=True,  # Enable mixed precision for faster training on GPUs
    learning_rate=5e-5,  # Learning rate for fine-tuning
    report_to="none",  # Disable reporting to external platforms
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
)

# Train the model
trainer.train()

import torch

# Ensure the model is on the correct device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Generate new session payloads
input_text = "Session hijack detected with unusual IP changes."
input_ids = tokenizer.encode(input_text, return_tensors="pt").to(device)  # Move input_ids to the same device as the model

# Generate text with sampling enabled
output = model.generate(
    input_ids,
    max_length=150,         # Maximum length of generated text
    num_return_sequences=5, # Generate 5 sequences
    temperature=0.7,        # Controls randomness
    top_k=50,               # Top-k sampling
    top_p=0.95,             # Nucleus sampling
    do_sample=True          # Enable sampling
)

# Decode and print generated payloads
generated_payloads = [tokenizer.decode(g, skip_special_tokens=True) for g in output]
for i, payload in enumerate(generated_payloads):
    print(f"Payload {i + 1}:\n{payload}\n")

"""# **modify3**"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.cluster import KMeans
from transformers import BertTokenizer

# Load dataset
data_path = '/content/drive/MyDrive/LDAP.csv'
df = pd.read_csv(data_path)

# Strip leading/trailing spaces from column names
df.columns = df.columns.str.strip()

# Check for relevant columns and adapt
required_columns = ['Flow ID', 'Source IP', 'Destination IP', 'Protocol']
missing_columns = [col for col in required_columns if col not in df.columns]
if missing_columns:
    raise KeyError(f"The following required columns are missing: {missing_columns}")

# Combine relevant columns into a single 'logs' column
df['logs'] = df[required_columns].astype(str).agg(' '.join, axis=1)

# Use 'Label' column for binary classification
if 'Label' not in df.columns:
    raise KeyError("'Label' column is missing in the dataset. Please check the file.")

# Map labels for binary classification
df['label'] = df['Label'].apply(lambda x: 1 if x in ['NetBIOS', 'LDAP'] else 0)

# Analyze Label Distribution
print("Label distribution:\n", df['label'].value_counts())

# 1. Feature Extraction: Extract patterns from the logs
vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=1000)  # Use unigrams and bigrams
X = vectorizer.fit_transform(df['logs'])

# Print top features for analysis
print("Top Features Extracted:", vectorizer.get_feature_names_out())

# 2. Cluster logs based on patterns
kmeans = KMeans(n_clusters=5, random_state=42)  # Adjust clusters based on expected patterns
df['cluster'] = kmeans.fit_predict(X)

# Analyze clusters
print("Cluster distribution:\n", df['cluster'].value_counts())
print(df[['logs', 'cluster']].head())

# Split data into training and test sets (if needed for classification)
train_texts, test_texts, train_labels, test_labels = train_test_split(
    df['logs'], df['label'], test_size=0.2, random_state=42
)

# Tokenize using BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=128, return_tensors="pt")
test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, max_length=128, return_tensors="pt")

print("Tokenization complete. Data ready for deeper analysis and simulation.")

# Save the cluster-wise analysis for generating new attack payloads
cluster_analysis = df.groupby('cluster')['logs'].apply(list)

# Simulate unique payloads using GPT
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Load GPT-2 model and tokenizer
gpt_tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
gpt_model = GPT2LMHeadModel.from_pretrained("gpt2")

# Generate new attack payloads for each cluster
for cluster_id, logs in cluster_analysis.items():
    print(f"\nGenerating payloads for Cluster {cluster_id}:")
    example_prompt = f"An example of session logs for cluster {cluster_id}: {logs[0]}"
    inputs = gpt_tokenizer.encode(example_prompt, return_tensors="pt")

    # Generate text
    outputs = gpt_model.generate(
        inputs,
        max_length=50,
        num_return_sequences=3,
        temperature=0.7,
        do_sample=True
    )
    generated_logs = [gpt_tokenizer.decode(output, skip_special_tokens=True) for output in outputs]

    for i, log in enumerate(generated_logs):
        print(f"Generated Log {i + 1}: {log}")

"""Another Model:Distil:GPT-2"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.cluster import KMeans
from transformers import BertTokenizer, GPT2Tokenizer, GPT2LMHeadModel

# Load dataset
data_path = '/content/drive/MyDrive/LDAP.csv'
df = pd.read_csv(data_path)

# Strip leading/trailing spaces from column names
df.columns = df.columns.str.strip()

# Check for relevant columns and adapt
required_columns = ['Flow ID', 'Source IP', 'Destination IP', 'Protocol']
missing_columns = [col for col in required_columns if col not in df.columns]
if missing_columns:
    raise KeyError(f"The following required columns are missing: {missing_columns}")

# Combine relevant columns into a single 'logs' column
df['logs'] = df[required_columns].astype(str).agg(' '.join, axis=1)

# Use 'Label' column for binary classification
if 'Label' not in df.columns:
    raise KeyError("'Label' column is missing in the dataset. Please check the file.")

# Map labels for binary classification
df['label'] = df['Label'].apply(lambda x: 1 if x in ['NetBIOS', 'LDAP'] else 0)

# Analyze Label Distribution
print("Label distribution:\n", df['label'].value_counts())

# 1. Feature Extraction: Extract patterns from the logs
vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=1000)  # Use unigrams and bigrams
X = vectorizer.fit_transform(df['logs'])

# Print top features for analysis
print("Top Features Extracted:", vectorizer.get_feature_names_out())

# 2. Cluster logs based on patterns
kmeans = KMeans(n_clusters=5, random_state=42)  # Adjust clusters based on expected patterns
df['cluster'] = kmeans.fit_predict(X)

# Analyze clusters
print("Cluster distribution:\n", df['cluster'].value_counts())
print(df[['logs', 'cluster']].head())

# Tokenize using BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

train_texts, test_texts, train_labels, test_labels = train_test_split(
    df['logs'], df['label'], test_size=0.2, random_state=42
)
train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=128, return_tensors="pt")
test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, max_length=128, return_tensors="pt")

print("Tokenization complete. Data ready for deeper analysis and simulation.")

# Save the cluster-wise analysis for generating new attack payloads
cluster_analysis = df.groupby('cluster')['logs'].apply(list)

# Simulate unique payloads using DistilGPT-2
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load DistilGPT-2 tokenizer and model
gpt_tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
gpt_model = AutoModelForCausalLM.from_pretrained("distilgpt2")

# Generate more attack payloads for each cluster
for cluster_id, logs in cluster_analysis.items():
    print(f"\nGenerating payloads for Cluster {cluster_id}:")
    example_prompt = f"An example of session logs for cluster {cluster_id}: {logs[0]}"
    inputs = gpt_tokenizer.encode(example_prompt, return_tensors="pt")

    # Generate text
    outputs = gpt_model.generate(
        inputs,
        max_length=100,             # Increased max length
        num_return_sequences=10,    # Generate 10 sequences per cluster
        temperature=0.7,            # Controls randomness
        top_k=50,                   # Top-k sampling
        top_p=0.95,                 # Nucleus sampling
        do_sample=True              # Enable sampling for diverse outputs
    )
    generated_logs = [gpt_tokenizer.decode(output, skip_special_tokens=True) for output in outputs]

    for i, log in enumerate(generated_logs):
        print(f"Generated Log {i + 1}:\n{log}")